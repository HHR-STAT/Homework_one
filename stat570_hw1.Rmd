---
title: "Homework 1"
author: "Helene, Rui, Hong"
date: "September 12, 2016"
output: 
  html_document:
    toc: yes
    toc_depth: 4
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=4, fig.height=3.5, fig.path='Figs/',
                      echo=TRUE, warning=FALSE, message=FALSE)
options(scipen=1, digits=2)
```

### Question 1

### Question 2: The Radio Survey Dataset

#### a) Import and clean the data


 Read the data into R 
```{r, import, echo = F}
## Clear the R environment, and import necessary packages. 
rm(list=ls())
library(plyr)
library(ggplot2)
library(sqldf)

dir <- "C:/Users/rpei.JANUS/Desktop/stat_571_hw1/data/" # this should be the folder containing data.
setwd(dir)
#survey_raw <- read.csv("/Users/Rui/Dropbox/Upenn/Stat571/Data/Survey_results_final.csv")
survey_raw <- read.csv("Survey_results_final.csv")

```

Extract only the variables of interest.
```{r extract_variables}
cols_keep <- c("WorkTimeInSeconds", "Answer.Age", "Answer.Education", "Answer.Gender", "Answer.HouseHoldIncome", "Answer.Sirius.Radio", "Answer.Wharton.Radio")
survey <- survey_raw[cols_keep]
colnames(survey) <- c("worktime", "age", "edu", "gender", "income", "sirius", "wharton")
```



```{r cleaning }
## cleaning the gender variable
print("Answers given to Gender.")
levels(survey$gender)
survey <- survey[-c(which(survey$gender == "")),]
## cleaning the age variable
print("Answers given to Age.")
levels(survey$age)
survey <- survey[-which(survey$age == ""),]
survey <- survey[-which(survey$age == "female"),]
survey$age[which(survey$age == "223")] <- "23"
survey$age[which(survey$age == "27`")] <- "27"
survey$age[which(survey$age == "4")] <- "40"  # not sure if should delete this data point
survey$age[which(survey$age == "Eighteen (18)")] <- "18"
survey$age <- as.numeric(levels(survey$age))[survey$age]
## cleaning up the education variable
print("Ansers given to education")
unique(survey$edu)
survey <- survey[-which(survey$edu == "select one"),]
```

Clean the data

* Remove data points with gender ""

* Remove data points with age = "", "female"

* Replace mistyped age = "223", "4", "Eighteen (18)", "27`"

* Remove data points with education = "select one"


#### b) summarize all the variables collected 

**Gender **

```{r summarize_gender}
a <- summary(survey$gender)[2:3]
gender <- c("female", "male")
number <- c(a[1]/(a[1] + a[2]),a[2]/(a[1] + a[2]))
gender <- data.frame(gender, number)

p <- ggplot(gender, aes(x = 1, y = number, fill = gender)) + 
  geom_bar(stat="identity") +
  coord_polar(theta = 'y')
p <- p + 
  geom_bar(stat = "identity", color = 'black') +
  guides(fill=guide_legend(override.aes=list(colour=NA))) +
  theme(axis.ticks=element_blank(),  # the axis ticks
          axis.title=element_blank(),  # the axis labels
          axis.text.y=element_blank()) # the 0.75, 1.00, 1.25 labels.
print(p)
```

58% of the sample is male. 

**Work time **
```{r summarize_worktime}
summary(survey$worktime)
f1 <- ggplot(survey, aes(worktime, fill = gender)) + 
  geom_histogram(bins = 60) +
  xlab("Work Time (s)") + ylab("Count") + 
  ggtitle("Distribution of work time")
f1
```
**Age **
```{r summarize_age}
summary(survey$age)
sd(survey$age)
f2 <- ggplot(survey, aes(age, fill = gender)) + 
  geom_histogram(bins = 30) +
  xlab("Age") + ylab("Count") + 
  ggtitle("Distribution of participant age")
f2
```
**Education **
```{r summarize_education}
survey$edu <- revalue(survey$edu, c("Bachelor’s degree or other 4-year degree" = "Bachelor", 
                      "Graduate or professional degree" = "Graduate",
                      "High school graduate (or equivalent)" = "High school",
                      "Less than 12 years; no high school diploma" = "<12",
                      "Other" = "Other",
                      "Some college, no diploma; or Associate’s degree" = "Some college",
                      "select one" = "select one"   
                      ))
summary(survey$edu)
f <- ggplot(survey, aes(x = edu, fill = gender)) + geom_bar(position = "dodge")
print(f)
```

**Income **
```{r summarize_income}
survey$income_mean <- NA
survey$income_mean[which(survey$income == "$15,000 - $30,000")] <- mean(c(15000, 30000))
survey$income_mean[which(survey$income == "$30,000 - $50,000")] <- mean(c(30000, 50000))
survey$income_mean[which(survey$income == "$50,000 - $75,000")] <- mean(c(50000, 75000))
survey$income_mean[which(survey$income == "$75,000 - $150,000")] <- mean(c(75000, 150000))
survey$income_mean[which(survey$income == "Above $150,000")] <- (150000)
survey$income_mean[which(survey$income == "Less than $15,000")] <- (15000)
survey$income_mean <- as.numeric(survey$income_mean) 
table(survey$income_mean)

ggplot(survey,aes(income_mean, fill = gender)) + 
  geom_bar(position = "dodge")
```


### b) Sample properties
#### b.1) Sample vs. U.S. population
Does this sample appear to be a random sample from the general population of the U. S.? IF so we can get our estimate easily. (Utilize your control variables, compare them to sources such as the CIA world factbook).
```{r sample_US}
n <- length(survey$age) # total sample size
p1 <- length(which(survey$age <25))
p2 <- length(which(survey$age < 55)) - p1
p3 <- length(which(survey$age < 65)) - p1 - p2
p4 <- length(which(survey$age > 64))

df1 <- survey[which(survey$age < 25),]
d1 <- summary(df1$gender)
df2 <- survey[setdiff(which(survey$age < 55), which(survey$age < 25)),]
d2 <- summary(df2$gender)
df3 <- survey[setdiff(which(survey$age < 65), which(survey$age < 55)),]
d3 <- summary(df3$gender)
df4 <- survey[which(survey$age > 64),]
d4 <- summary(df4$gender)

```


Age group | CIA age structure (\%) | Sample age structure (\%) | CIA sex ratio (males/female) | Sample sex ratio (males/female) 
------------- | -------------  | -------------  | -------------  | ------------- 
0 - 14 years      | 18.99    | 0           | 1.04            | NA     
15 - 24 years     | 13.64   |    `r 100 * p1/n`     | 1.05      | `r  d1[3]/d1[2]`                    
25 - 54 years     | 39.76   |  `r 100 *p2/n`    | 1          |   `r d2[3]/d2[2]`
55 - 64 years     | 12.73     | `r 100 *p3/n`   | 0.93     |     `r d3[3]/d3[2]`                          
65 years and over | 14.88  |  `r 100 *p4/n` | 0.79            |   `r d4[3]/d4[2]`                        

#### b 2) Sample vs. mTurk population
Does this appear to be a random sample from mTurk population?

We downloaded demographic data of 5000 most recent mTurk surveys from https://mturk-surveys.appspot.com/, and compared this demographic information with our sample. 
```{r sample_mturk}
setwd(dir)
mturk <- read.csv("mturk_surveys.csv")

n_mturk <- length(mturk$Age)
mf1 <- mturk[which(mturk$Age < 25),]
m1 <- summary(mf1$gender)
mf2 <- mturk[setdiff(which(mturk$Age < 55),which(mturk$Age < 25)),]
m2 <- summary(mf2$gender)
mf3 <- mturk[setdiff(which(mturk$Age < 65),which(mturk$Age < 55)),]
m3 <- summary(mf3$gender)
mf4 <- mturk[which(mturk$Age > 64),]
m4 <- summary(mf4$gender)

```

Age group | MTurk age structure (\%) | Sample age structure (\%) | MTurk sex ratio (males/female) | Sample sex ratio (males/female) 
------------- | -------------  | -------------  | -------------  | ------------- 
16 - 24 years     |  `r 100*length(mf1$Age)/n_mturk`  |   `r 100 * p1/n`     | `r  m1[2]/m1[1]` | `r  d1[3]/d1[2]`                    
25 - 54 years     |  `r 100*length(mf2$Age)/n_mturk`  |   `r 100 * p2/n`   | `r  m2[2]/m2[1]`    |   `r d2[3]/d2[2]`
55 - 64 years     | `r 100*length(mf3$Age)/n_mturk`    |  `r 100 * p3/n` |`r  m3[2]/m3[1]`    |     `r d3[3]/d3[2]`                          
65 years and over |  `r 100*length(mf4$Age)/n_mturk` |  `r 100 * p4/n` | `r  m4[2]/m4[1]`   |   `r d4[3]/d4[2]`  


This table suggests that our sample is a fairly good representation of Amazon Mechanical Turk U.S. population.

#### b  3) Estimating U.S. Wharton radio listeners.
Assume that the proportion of Wharton listeners vs. that of Sirius listeners remains the same in the general population as it is in the MTURK population. Use the data to provide an estimate of the number of Wharton listeners in the U. S. In order to make this estimate do you need to break down the proportion of Wharton to Sirius by age? Provide some graphical or numerical evidence to support your reasoning.

We break down the proportion of Wharton to Sirius listeners by age.  **Here we are assuming that sample represents MTurk population **

Age group |  CIA age structure (\%) | MTurk age structure (\%) |  Wharton to Sirius listeners (\%)
------------- | -------------  | -------------  |  ------------- 
16 - 24 years    | 13.64  |  `r 100*length(mf1$Age)/n_mturk`  | `r summary(df1$wharton)[3]/summary(df1$sirius)[3]`                   
25 - 54 years    | 39.76  |  `r 100*length(mf2$Age)/n_mturk`  | `r summary(df2$wharton)[3]/summary(df2$sirius)[3]`
55 - 64 years    | 12.73 | `r 100*length(mf3$Age)/n_mturk`   |  `r summary(df3$wharton)[3]/summary(df3$sirius)[3]`
65 years and over | 14.88 |  `r 100*length(mf4$Age)/n_mturk`  |  `r summary(df4$wharton)[3]/summary(df4$sirius)[3]`

One approach is to estimate the proportion of the Wharton listeners to that of the Sirius listeners, p, so that we will come up with an audience size estimate of approximately 51.6 million times p.

Hence, we estimate the proportion of the Wharton listeners to that of the Sirius listeners in the U.S 
$$ p = 13.64\% \times 0.06 + 39.76\% \times 0.05 + 12.73\% \times 0.03 + 14.88\% \times 0.14 $$
p = `r 0.1364*0.06 + 0.3976*0.05 + 0.1273*0.03 + 0.1488*0.14`

Number of wharton listeners =  $p \times 51.6 =  2.6 million$ 






### Question 3 
# a. i) 
Generate this sample of (x,y) pairs and make a scatter plot:
x = seq(0,1,length=40)
y = vector(mode = "numeric", length = 40)
for (i in 1:40){
y[i] = 1 + 1.2*x[i] + rnorm(1,0,sd=2)}
plot(x,y)
# ii) 
Fit an lm model and draw the line over the plot above
```
fitlm = lm(y~x)
summary(fitlm)
```

graphical representation:
```
abline(fitlm, col="red", lwd=4)
```
The estimated beta0 is 1.3945, estimated beta1 is 0.5286. The estimated equation is y = 1.3945 + 0.5286*x The 95% confindence interval for beta1 is (-1.4447320 2.501886), which captures the true beta1. The RSE is 1.825, which is reasonably close to 2.

# b. i) 
The function I used for generating the samples:
```
simlm = function(samplesize, nosim, b0, b1, errsd){
  #This function simulate an experiment with user defined sample size, number of trials, true coefficients and
  #standard deviations of the relation. For this homework assignment, 100 trials of sample size 40 experiments
  #with the parameters specified were experimented.
  x = seq(0,1,length=samplesize) #The values of x are fixed.
  b0_hat = vector(mode="numeric",length=0) #Vectors to be used to store the
  b1_hat = vector(mode="numeric",length=0) #final results
  
  b0_ci_ub = vector(mode="numeric",length=0)
  b0_ci_lb = vector(mode="numeric",length=0)
  
  b1_ci_ub = vector(mode="numeric",length=0)
  b1_ci_lb = vector(mode="numeric",length=0)
  
  for (k in 1:nosim){ 
    #repeat a number of times with the specified parameters
    y = b0 + b1*x + rnorm(samplesize, sd=errsd)
    lm_model = lm(y~x)
    coef = summary(lm_model)$coefficients
    b0_hatc = coef[1,1]; b0_se = coef[1,2]; #estimates of the current run
    b1_hatc = coef[2,1]; b1_se = coef[2,2];
    
    t = qt(0.975,df=samplesize-2)
    b0_ci_ubc = b0_hatc + t*b0_se; b0_ci_lbc = b0_hatc - t*b0_se
    b1_ci_ubc = b1_hatc + t*b1_se; b1_ci_lbc = b1_hatc - t*b1_se
    
    b0_hat = append(b0_hat, b0_hatc)
    b1_hat = append(b1_hat, b1_hatc)
    b0_ci_ub = append(b0_ci_ub, b0_ci_ubc)
    b0_ci_lb = append(b0_ci_lb, b0_ci_lbc)
    b1_ci_ub = append(b1_ci_ub, b1_ci_ubc)
    b1_ci_lb = append(b1_ci_lb, b1_ci_lbc)
  }
  result = cbind(b0_hat, b1_hat, b0_ci_ub, b0_ci_lb, b1_ci_ub, b1_ci_lb)
}
samp = simlm(40,100,1,1.2,2)
```
This function returns a table of the estimated parameters and CIs for the 100 trials.
# ii) summary of the LS estimates of beta1
```
beta1 = samp[,2]
hist(beta1, breaks=10,col=rgb(1,0.5,0.3),  main="The distribution of estimated beta1")
```
The distribution of beta1 seems to center around 1.3, which is pretty close to the true beta1.

# iii) 
There are 97 times that my 95% CI convered the true beta1 in this simulation.
The confidence intervals are plotted in the figure below. This part is also integrated into a function like above for the sake of reproduceability.
```
plot_sim = function(samp, b1){
#The input argument "samp" is a simlm object. The value of true b1, used in the original simulation, should also be provided.
  bi_lb = samp[,6]
  bi_ub = samp[,5]
  x = seq(nrow(samp))
  plot(x,bi_lb, ylim=c(min(bi_lb),max(bi_ub)), col="red",cex=0.5, ylab="",xlab="")
  par(new=T)
  plot(x,bi_ub, ylim=c(min(bi_lb),max(bi_ub)), col="green",cex=0.5,ylab="Confidence Interval", xlab="number of trials")
  segments(x, bi_lb, x, bi_ub, lwd=0.3, lty=2)
  abline(a=b1, b=0, col="dark blue", lty=3, lwd=2.5)
}
plot_sim(samp,1.2)
```
If, say, I try 500 simulations with the same parameters:
```
sim500 = simlm(40,500,1,1.2,2)
plot_sim(sim500,1.2)
```
We can find how many times the calculated CI fails to capture the true beta1 with the following code:
```
ci_out = function(samp,b1){
#The input of the function is a simlm object, with the true beta1 fed into the simlm function
#The function returns the percentage of trials in which CI fails to capture the true beta1
cib1 = samp[,5:6] #extract upper and lower bound of CI
countv = vector(mode="numeric",length=nrow(cib1))
for (i in 1:nrow(cib1)){
if (samp[i,5]<b1 | samp[i,6]>b1){
countv[i]=1
} else {countv[i]=0}
}
ciout = sum(countv)
out_ratio = (ciout/nrow(cib1))*100
}
```
With this set of functions, we can try multiple user-defined simulations, and plot the change of the failure rate. Say we do the simluations with increasing sample size (from 100 to 10,000):
```
numsim = 100
simsize = seq(numsim)
result = vector(mode="numeric",length=numsim)
for (i in 1:numsim){
sim = simlm(10*i,100,1,1.2,2)
outs = ci_out(sim,1.2)
result[i] = outs
}
plot(simsize, result, col="blue", cex=1, xlab="Sample size/10", ylab="Percentage of CI's that fail to capture the true mean")
```
The result appears to be pretty random. Therefore there isn't a relation between sample size and whether the CI derived from the sample could capture true mean.



### Question 4 : Major League Baseball

####i) Summarize the data by providing suitable summary statistics and graphs. Write a brief report for your findings.

#####Load and examine dataset:
```{r 4.a.1}
setwd(dir)
paydata <- read.csv("MLPayData_Total.csv")
names(paydata)

#####select the variables we are interested in: team names, team total payroll from 1998 to 2014 (in billions), fraction of games won from 1998 to 2014, and payroll (in millions) and fraction of games won for the years we want to examine:  
paydata1 <- paydata[,c(3,1:2,4,54,12,46,20,38)]
head(paydata1)

#####rename the variables to more intelligible names: 
names(paydata1) <- c("team","tot.payroll","avgwin","payroll.1998","pctwin.1998","payroll.2006","pctwin.2006","payroll.2014","pctwin.2014")
head(paydata1)

#####create a vector of the mean of each variable, beside team 
tot.mean <- apply(X = paydata1[,-1], MARGIN = 2, FUN = mean)

#####create tot.mean row with the mean for all variables............. work in progress.................. it's not working!!!!!!! 
team <- "tot.mean"
tot.mean <- rbind(team, tot.mean)

#paydata2 <- rbind(paydata1, tot.mean)

tot.mean <- c("tot.mean",tot.mean)
paydata1[,1]

#####display the observations for the teams we are interested in (Boston Red Sox & Oakland A's), plus the first three teams in the dataset.  
paydata1[c(4,20,1,2,3),]

#####boxplots for tot.payroll and avgwin 
boxplot(paydata$payroll)
######WORK IN PROGRESS......trying to put a horizontal line for A's and Red Sox 
```
  abline(
    h = 0.5487172,
    col = "lightblue",
    lwd = 2 
  )
```{r 4}
######END OF WORK IN PROGRESS....... 
boxplot(paydata$avgwin)

#####scatterplot of average by total payroll from 1998 to 2014 
plot(paydata$payroll, paydata$avgwin, 
       pch  = 16, 
       cex  = 1,
       col  = "blue",
       xlab = "Payroll", 
       ylab = "Win Percentage",
       main = "MLB Teams's Overall Win Percentage vs. Payroll")

######WORK IN PROGRESS: identify the Oakland A's and Boston Red Sox in the scatterplot. 
```
identify(paydata$payroll, paydata$avgwin, labels=paydata$team, plot=TRUE) 

####ii) For a given year, is payroll a significant variable to predict the winning percentage of that year? Choose a year and run a regression to examine this. You may try this for a few different years. Explain your results.

  myfit0 <- lm(avgwin~tot.payroll, data=paydata1)
  summary(myfit0)
  
  myfit1998 <- lm(pctwin.1998~payroll.1998, data=paydata1)  
  summary(myfit1998)
  
  myfit2006 <- lm(pctwin.2006~payroll.2006, data=paydata1)  
  summary(myfit2006)
  
  myfit2014 <- lm(pctwin.2014~payroll.2014, data=paydata1)  
  summary(myfit2014)
  
  par(mfrow = c(2,2))
  
    plot(paydata1$tot.payroll, paydata$avgwin, 
       pch  = 16, 
       cex  = 1.2,
       col  = "blue",
       xlab = "Payroll", 
       ylab = "Win Percentage",
       xlim = c(0, 2.5),
       ylim = c(0.4, 0.7),
       main = "MLB Teams's Overall Win Percentage vs. Payroll")
    abline(myfit0, col="red", lwd=4)
    
    plot(paydata1$payroll.1998, paydata1$pctwin.1998, 
       pch  = 16, 
       cex  = 1.2,
       col  = "blue",
       xlab = "Payroll", 
       ylab = "Win Percentage",
       xlim = c(0, 250),
       ylim = c(0.4, 0.7),
       main = "MLB Teams's Win Percentage vs. Payroll in 1998")
    abline(myfit1998, col="red", lwd=4) 

    plot(paydata1$payroll.2006, paydata1$pctwin.2006, 
       pch  = 16, 
       cex  = 1.2,
       col  = "blue",
       xlab = "Payroll", 
       ylab = "Win Percentage",
       xlim = c(0, 250),
       ylim = c(0.4, 0.7),
       main = "MLB Teams's Win Percentage vs. Payroll in 2006")
    abline(myfit2006, col="red", lwd=4) 
  
    plot(paydata1$payroll.2014, paydata1$pctwin.2014, 
       pch  = 16, 
       cex  = 1.2,
       col  = "blue",
       xlab = "Payroll", 
       ylab = "Win Percentage",
       xlim = c(0, 250),
       ylim = c(0.4, 0.7),
       main = "MLB Teams's Win Percentage vs. Payroll in 2014")
    abline(myfit2014, col="red", lwd=4) 
  
####iii) Now use the aggregated information, and use regression to analyze the relation between total payroll and overall winning percentage. Run appropriate model(s) to answer the following questions.
#####a) In this analysis do the Boston Red Sox perform reasonably well given their total payroll? [Use a 95% prediction interval.]

  attach(paydata1)
  
  new <- data.frame(tot.payroll=c(1.972359)) #Boston Red Sox's total payroll 
  CImean <- predict(myfit0, new, interval="prediction", se.fit=TRUE)  
  CImean

######Answer: The Boston Red Sox, with an average of 55% of games won between 1998 and 2014, falls within the confidence interval in terms of their performance given their payroll.

#####b) In view of their winning percentage, how much payroll should the Oakland A’s have spent? [Use a 95% prediction interval.]

  myfit1 <- lm(tot.payroll~avgwin, data=paydata1)
  summary(myfit1)

  new2 <- data.frame(avgwin=c(0.5445067)) #Oakland A's average of games won between 1998 and 2014
    CImean <- predict(myfit1, new2, interval="prediction", se.fit=TRUE)  
    CImean

######Answer: Using the regression model in myfit1, we would have predicted that a team winning 54% of all games between 1998 and 2014 should have a total payroll over that time between 0.95 and 2.27 billion dollars. Given this result, we can say that the Oakland A's performed beyond expectation for their budget. 

#####iv) What is the best model you could build on the basis of this data to capture the important factors affecting winning percentage in 2014 and predict that percentage for 2015? 
#####Describe the process of your analysis. What criterion are you using? Report your findings.

  myfit2014 <- lm(pctwin.2014~payroll.2014, data=paydata1)  
  summary(myfit2014)

  attach(paydata1)
  
  new3 <- data.frame(payroll.2014=c(162.8174)) #Boston Red Sox's payroll for 2014
  CImean <- predict(myfit2014, new3, interval="prediction", se.fit=TRUE) #Prediction of Boston Red Sox's performance in 2015 based on the myfit2014 model, and its payroll in 2014 
  CImean
  
######I really don't know whether this is the right way. Please let me know if you have a better idea! 




### Question 5 
# a) i) to get some information about the data
```
library(ISLR)
help(Auto)
```

# ii) exploring the data using pairwise plot
```
pairs(Auto)
```
Summary statistics. The result is a table containing the mean, sd, and 25%, 50% and 75% quantiles of each variable
```
summary = data.frame(matrix(0, nrow=5, ncol=9))
for (i in 1:9){
ave = mean(Auto[,i])
qt = as.numeric(quantile(Auto[,i]))
std = sd(Auto[,i])
summary[1,i]= ave; summary[2,i]=std; summary[3,i]=qt[2]; summary[4,i]=qt[3]
summary[5,i]=qt[4]}
colnames(summary) = colnames(Auto)
row.names(summary) = c("mean","sd","25% quantile", "50% quantile", "75% quantile")
```

*anything interesting??* The relation between mpg and several predictors, such as diplacement, acceleration, weight and horsepower may be better modeled with a log transformed linear model. There also appears to have a lot of colinearity among weight, acceleration, horsepower and displacement.

# b) The time effect over mpg:
First, we constructed a simple linear regression model with time as the single predictor and mpg as response
```
mdl1 = lm(mpg~year, data=Auto)
summary(mdl1)
```

This model shows that year is a significant predictor over mpg, with about 1.23 mpg increase a year on average. The model explains about 33 percent of the variation of the data.

The second model also considers the effect of horse power over mpg, in addition ofyear
```
mdl2 = lm(mpg~year+horsepower, data=Auto)
summary(mdl2)
```

In this model, year is still a significant predictor at 0.05 level. However, mpg is only predicted to be increasing by about 0.6573 mpg per year, given a same horsepower of the car.

The CI's of each model are shown below:
```
confint(mdl1)

confint(mdl2)
```

The confidence interval of the first model is (1.058,1.402) and for the second model is (0.527,0.788). The difference is from the different model structure. The first model only considered the effect from time, while ignoring the others. However, the effect from time might be dependent upon effects from other factors, such as the weight of the vehicle and horsepower. When a second predictor is added, namely the horsepower in the second model, we restricted horsepower for the interpretation of time effect. Thus the time in the latter model is interpreted only when horsepower is held constant (at a single level)

The third model for this problem included an interaction term in the model:
```
mdl3 = lm(mpg~year*horsepower, data=Auto)
summary(mdl3)
```

The interaction term in this model is significant at 0.05 level. This interaction thus tells us that the effect of either predictor on the response is dependent on the other. In this case, the effect of year, as shown in the model, is the effect of year when horsepower is set to zero. Therefore, when horsepower is zero, or has no effect on mpg, mpg is about to increase 2.192 units per year.

# c) 
The same variable that plays different roles is the number of cylinders.
# i) 
The first model treats the number of cylinders as continuous variable:
```
mdl4 = lm(mpg~horsepower + cylinders, data=Auto)
summary(mdl4)
```

This model shows that cylinder is a significant predictor at 0.01 level. The increase in one cylinder is associated with 1.91982 unit decrease in mpg given a certain horsepower.

# ii) 
The second model fitted here treats the number of cylinders as categorical variable:
```
mdl5 = lm(mpg~horsepower + factor(cylinders), data=Auto)
summary(mdl5)
```

The model shows that when the car has four cylinders, the fuel efficiency will increase on average about 6.57344 mpg. This effect is signigicant at 0.01 level. However, given a same horsepower, other number of cylinders do not appear to have effects on the car's fuel efficiency.
```
anova(mdl4,mdl5)
```

The F-test shows that the number of cylinders as categorical predictors is a significant predictor of fuel efficiency.

# iii) 
The fundamental difference between the two models is their treatment of cylinders. In the first model, the number of cylinders was treated as a continuous variable. This treatment assumes the number of cylinders as one homogenous variable, while ignores the inner structure and qualitive difference between cars with different number of cylinders. On the other hand, treating the number of cylinders as discrete, and independent variables encodes the ideocyncratic properties of each configuration of the engine (number of cylinders). Therefore the final model returns that only 4 cylinder cars have significantly better fuel efficiency with given horsepower, while other designs don't.

# d) The final model
We can start building the final model from building a full model with all the predictors.
```
full_mdl = lm(mpg ~ factor(cylinders) + displacement + horsepower + weight + acceleration + year + 
    factor(origin), data=Auto)
summary(fmdl)
```
From here, we then perform model selection to find the optimal model. The exhaustive search algorithm is implemented using the leaps package here.
```
library(leaps)
fit.exh = regsubsets(mpg~factor(cylinders) + displacement + horsepower + weight + acceleration + year + 
    factor(origin), Auto, nvmax = 15, method="exhaustive")
f.e = summary(fit.exh)
```
We first search for our optimal model using Cp:
```
plot(f.e$cp, xlab="Number of predictors", ylab="cp", col="green", type="p",pch=16)
opt.size = which.min(f.e$cp)
opt.size
fit.exh.var <- f.e$which
fit.exh.var[opt.size,]
```
According to the Cp method, the optimal size of the model is 10, with a model including all the predictors but acceleration.

Then we proceed with the BIC method:
```
plot(f.e$bic, xlab="Number of predictors", ylab="bic", col="red", type="p",pch=16)
opt.sizeb = which.min(f.e$bic)
fit.exh.var[opt.sizeb,]
```
The BIC method returns that the best model should include all the predictors but acceleration, also excluding categorical variables 6 and 8 sylinder car.

The diagnostic plot at the very beginning of this problem suggests that the relation between mpg and other predictors might be logrithmic. Therefore the second attempt is to log-transform mpg and fit a model predicting the logrithmic of mpg.
```
car = Auto
lmpg = log(car$mpg)
car$mpg = lmpg
logmdl = lm(mpg~factor(cylinders)+displacement+horsepower+weight+acceleration+year+factor(origin), data=car)
summary(logmdl)
```

Then we proceed with the same model selection procedure with this new model:
```
new.exh = regsubsets(mpg~factor(cylinders) + displacement + horsepower + weight + acceleration + year + 
    factor(origin), car, nvmax = 15, method="exhaustive")
new.e = summary(new.exh)
plot(new.e$cp, xlab="Number of predictors", ylab="cp", col="green", type="p",pch=16)
opt.size = which.min(new.e$cp)
opt.size
plot(new.e$bic, xlab="Number of predictors", ylab="bic", col="red", type="p",pch=16)
opt.sizeb = which.min(new.e$bic)
opt.sizeb
new.exh.var <- new.e$which
new.exh.var[opt.size,]
new.exh.var[opt.sizeb,]

```
This time, the best model returned by Cp method contains all the predictors but acceleration and 8 cylinders, and the best model returned by BIC contains categorical predictor 4-cylinder and everything else but accelearion. Combining the results from the two methods, the final model with log transformation should be of the form:
lmpg ~ cylinders + weight + displacement + horsepower + year + origin
in which cylinders and origin are categorical variables.

The final attempt also includes the interaction between the terms displacement and horsepower, displacement and weight, horsepower and weight, and horsepower and acceleration. Therefore the starting full model is the following:
```
int.mdl = lm(lmpg ~ factor(cylinders) + factor(origin) + displacement*horsepower + displacement*weight +   horsepower*weight + horsepower*acceleration + year, data=car)
summary(int.mdl)
```
The model selection still follows the same routine as before.
```
int.exh = regsubsets(lmpg~factor(cylinders) + factor(origin) + displacement*horsepower + displacement*weight +   horsepower*weight + horsepower*acceleration + year, car, nvmax = 15, method="exhaustive")
int.e = summary(int.exh)
plot(new.e$cp, xlab="Number of predictors", ylab="cp", col="green", type="p",pch=16)
opt.size = which.min(int.e$cp)
opt.size
plot(new.e$bic, xlab="Number of predictors", ylab="bic", col="red", type="p",pch=16)
opt.sizeb = which.min(int.e$bic)
opt.sizeb
int.exh.var <- int.e$which
int.exh.var[opt.size,]
int.exh.var[opt.sizeb,]
```
The selected model by Cp contains 12 predictors: 4 sylinders, 5 cylinders, 6 sylinders, 8 cylinders, one origin, displacement, horsepower, weight, acceleration, year, the interaction term displacement\*weight and horsepower\*acceleration.

On the other hand, BIC returns a much smaller model, with only 4 cylinder, weight, acceleration, year, and the interaction terms horsepower:weight and horsepower:acceleration.

Combining the results from the two methods, we choose to retain cylinders, displacement, origin, weight, year and the interaction terms horsepower:weight and horsepower:acceleration in the final model.

Therefore, the three selected models with considerations outlined above are respectively:
```
fmdl1 = lm(mpg ~ factor(cylinders) + displacement + horsepower + weight + year + factor(origin), Auto)
fmdl2 = lm(mpg~factor(cylinders) + displacement + horsepower + weight + year + factor(origin), car)
fmdl3 = lm(mpg~factor(cylinders) + displacement + weight + year + factor(origin) +horsepower*weight + horsepower*acceleration, car)
summary(fmdl1); summary(fmdl2); summary(fmdl3);
```
Since fmdl3's performance is slightly better, in terms of RSE and adjusted R square, this model will be used for further prediction. The following code is used for prediction. Since the color and length of the new car are redundant information, they are discarded from the input vector for prediction.

Interpretation of the selected model:
The selected models tells us that when controlled for all other factors, compared to a 3-cylinder car, 4-cylinder cars have about 0.2696 unit of increase in log(mpg), 5-cylinder cars have 0.3157 more log(mpg), 6-cylinder cars have 0.211 more log(mpg) and 8-cylinder cars have 0.2204 more log(mpg). The cylinder configurations are significant at 0.01 level.

With regard to weight, increase in one pound is associated with a decrease of 2.2275\*10^-4 log(mpg), controlled for other factors. There is also a yearly improvement of about 0.02931 log(mpg). Compared to origin 1 (the US), origin two has 0.02642 more log(mpg), while it's not significant at 0.1 level. Origin 3 has 0.0518 more log(mpg) and is significant at 0.01 level. The increase of one horsepower is associated with a decrease of 0.005649 log(mpg). This effect is not significant at 0.1 level. The increase in one unit of acceleration is also associated with 0.001632 unit increase of log(mpg) and the effect is significant at 0.01 level. The interaction of weight and horsepower, as well as horsepower and acceleration, are also significant at 0.01 level, with an effect of 6.165\*10^-7 and -2.914\*10^-4 on log(mpg) respectively.

```
newcar = car[1,]
newcar[1]=NA; newcar[2:5]=c(8,350,260,4000); newcar[6]=0; newcar[7:8]=c(83,1); newcar[9]=NA
predc = predict(fmdl3, newcar, interval = "confidence", se.fit=T)
predp = predict(fmdl3, newcar, interval = "prediction", se.fit=T)
```

The result is as the following:
```
predc
```

```
predp
```

This result tells us that for a given car to be produced with the given specification, the 95% CI for the mean mpg is c(3.024848, 3.429058), and the 95% prediction interval is c(2.933668, 3.520237).

Examining the diagnostic plot:
```
#The residual plot
plot(fmdl3$fitted.values, fmdl3$residual, pch=16)
abline(h = 0, lwd = 5, col="red")
qqnorm(fmdl3$residuals)
qqline(fmdl3$residuals)
```
The symmetry of residuals with regard to h=0 suggests that the asumption of linearity is satisfied. The relatively evenly spread of residuals against fitted value indicates that homoscedasticity is ok. QQ plot shows a reasonably fitted line, which indicates normal errors. Thus the normal assumption of the error terms is also satisfied.

###Extra
Whether the variable "name" should be considered in the model?
Here, since the manufacture of cars can be a predictor for fuel efficiency, the following discussion will be about testing whether car maker is a good predictor.

First, the values in the column "name" are sorted by car maker. Since the name of cars is structured as "maker model", this step can be easily accomplished by the following code:
```
cname = Auto$name #make the names of cars a separate vector
cn = vector(mode="character",length=length(cname)) #an empty vector to store the sorted data
len = length(cname)
for (i in 1:len){
  cn[i] = regmatches(as.character(cname[i]), regexpr("[a-z]*",as.character(cname[i])))
  }
```

By summing up the sorted car names, we can see that the names do need some clean-up:
```
table(cn)

```
```
#to replace the apparent errors:
ind = which(cn %in% "toyota")
ind = which(cn %in% "toyouta")
cn[ind] = "toyota"
ind = which(cn %in% "vw")
cn[ind] = "volkswagen"
ind = which(cn %in% "vokswagen")
cn[ind] = "volkswagen"
ind = which(cn %in% "chevy")
cn[ind] = "chevrolet"
ind = which(cn %in% "chevroelt")
cn[ind] = "chevrolet"
ind = which(cn %in% "maxda")
cn[ind] = "mazda"

#Then put this column back to the original data set
car$name = cn
```
Then we can fit the cleaned data into a linear model. Two compare this model with the final model we selected, we can firt try to fit it with the same model specification:
```
cmdl = lm(mpg~factor(cylinders) + displacement + weight + year + factor(origin) + horsepower*weight + horsepower*acceleration + factor(name), car)
summary(cmdl)
Anova(cmdl)
anova(fmdl3,cmdl)
```
The F-test shows that the variable "name" is significant at 0.01 level, meaning that at least some of the car makers are factors affecting the mpg of the car.







